{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fee8945-3ed2-4ca3-905f-4555ce332ac1",
   "metadata": {},
   "source": [
    "## Structured Streaming in Action:\n",
    "Let’s get to an applied example of how you might use Structured Streaming. For our examples,\n",
    "we’re going to be working with the Heterogeneity Human Activity Recognition Dataset. The\n",
    "data consists of smartphone and smartwatch sensor readings from a variety of devices—\n",
    "specifically, the accelerometer and gyroscope, sampled at the highest possible frequency\n",
    "supported by the devices. Readings from these sensors were recorded while users performed\n",
    "activities like biking, sitting, standing, walking, and so on. There are several different\n",
    "smartphones and smartwatches used, and nine total users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e445de7-4417-4507-8cc7-9a33608fdac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12445f0d-cd86-494f-bea9-5a152a6a68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3adb4a8-0894-485e-a944-874a3dd01646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E:\\AI-ITI\\Second_3_Months\\PySpark\\Book\\data\\activity_data\n",
    "PATH = \"data/activity_data/streaming_test.json\"\n",
    "static = spark.read.json(PATH)\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66947e36-7384-4001-be91-cef0ca74b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68edce72-f666-4a2f-bba8-34c511582ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+---------------+-----+-------------+------------+-------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|_corrupt_record|   gt|            x|           y|            z|\n",
      "+-------------+-------------------+--------+-----+------+----+---------------+-----+-------------+------------+-------------+\n",
      "|1424686735175|1424686733176178965|nexus4_1|   35|nexus4|   g|           null|stand| 0.0014038086|   5.0354E-4|-0.0124053955|\n",
      "|1424686735378|1424686733382813486|nexus4_1|   76|nexus4|   g|           null|stand|-0.0039367676| 0.026138306|  -0.01133728|\n",
      "|1424686735577|1424686733579072031|nexus4_1|  115|nexus4|   g|           null|stand|  0.003540039|-0.034744263| -0.019882202|\n",
      "+-------------+-------------------+--------+-----+------+----+---------------+-----+-------------+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab6528-8947-4e2d-b7fb-680f8faea835",
   "metadata": {},
   "source": [
    "* Basically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames.\n",
    "* However, one small difference is that Structured Streaming does not let you perform schema inference without explicitly enabling it. You can enable schema inference for this by setting the configuration spark.sql.streaming.schemaInference to true. \n",
    "* Given that fact, we will read the schema from one file (that we know has a valid schema) and pass the dataSchema object from our static DataFrame to our streaming DataFrame.\n",
    "* As mentioned, you should avoid doing this in a production scenario where your data may (accidentally) change out from under you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ce94ba-558b-4efd-82d3-7b6a252c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilePerTrigger\", 1)\\\n",
    ".json(\"data/activity_data/streaming_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9da1cd-df6d-4f4c-8828-961255788541",
   "metadata": {},
   "source": [
    "Just like with other Spark APIs, streaming DataFrame creation and execution is lazy. In\n",
    "particular, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "574703ae-227c-47ae-82fc-a59eb08a40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541c3b8-3c3f-4568-831f-a5a531d4fac4",
   "metadata": {},
   "source": [
    "Because this code is being written in local mode on a small machine, we are going to set the shuffle partitions to a small value to avoid creating too many shuffle partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3b7b19-16d0-468c-a5e6-89745f70e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0e085-4670-4e3a-9411-f5d222d81260",
   "metadata": {},
   "source": [
    "* Now that we set up our transformation, we need only to specify our action to start the query. \n",
    "* We will specify an output destination, or output sink for our result of this query. \n",
    "* For this basic example, we are going to write to a memory sink which keeps an in-memory table of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25b07bb-3d2d-40e2-800a-a4b2daef47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e11cc-2347-439a-95e4-5a08ba3ee40d",
   "metadata": {},
   "source": [
    "We are now writing out our stream! You’ll notice that we set a unique query name to represent\n",
    "this stream, in this case activity_counts. We specified our format as an in-memory table and\n",
    "we set the output mode.\n",
    "When we run the preceding code, we also want to include the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71465bc0-af0f-4aaa-9ca4-5a328a469eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Option 'basePath' must be a directory\n=== Streaming Query ===\nIdentifier: activity_counts [id = b3976ce6-3b84-4d17-bf36-232d0d983609, runId = 9db05c32-4947-4e76-b589-c3fe892badad]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/E:/AI-ITI/Second_3_Months/PySpark/Book/data/activity_data/streaming_test.json]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5b33f458\n+- Aggregate [gt#92], [gt#92, count(1) AS count#119L]\n   +- StreamingExecutionRelation FileStreamSource[file:/E:/AI-ITI/Second_3_Months/PySpark/Book/data/activity_data/streaming_test.json], [Arrival_Time#85L, Creation_Time#86L, Device#87, Index#88L, Model#89, User#90, _corrupt_record#91, gt#92, x#93, y#94, z#95]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c406b906d45f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mactivityQuery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: Option 'basePath' must be a directory\n=== Streaming Query ===\nIdentifier: activity_counts [id = b3976ce6-3b84-4d17-bf36-232d0d983609, runId = 9db05c32-4947-4e76-b589-c3fe892badad]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/E:/AI-ITI/Second_3_Months/PySpark/Book/data/activity_data/streaming_test.json]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5b33f458\n+- Aggregate [gt#92], [gt#92, count(1) AS count#119L]\n   +- StreamingExecutionRelation FileStreamSource[file:/E:/AI-ITI/Second_3_Months/PySpark/Book/data/activity_data/streaming_test.json], [Arrival_Time#85L, Creation_Time#86L, Device#87, Index#88L, Model#89, User#90, _corrupt_record#91, gt#92, x#93, y#94, z#95]\n"
     ]
    }
   ],
   "source": [
    "activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e7afa-c5c7-4753-9a70-36eb7199ac48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
