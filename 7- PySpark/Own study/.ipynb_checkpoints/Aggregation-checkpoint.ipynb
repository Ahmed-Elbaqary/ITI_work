{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93699798-97e6-4c27-86b7-87de7c8189ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54b48b1-ad43-486f-89d1-217357ff4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d09371-367b-4b99-a447-4f40e8730cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"Book_Exercises/data/csv/all.txt\")\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25489f3c-ccfb-4a56-82bb-54a6a8e2f023",
   "metadata": {},
   "source": [
    "**What is Coalesce?** \\\n",
    "The coalesce method reduces the number of partitions in a DataFrame. Coalesce avoids full shuffle, instead of creating new partitions, it shuffles the data using Hash Partitioner (Default), and adjusts into existing partitions, this means it can only decrease the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9400d5f-863f-434c-a446-1cdc19f395cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d552ebb-f480-446c-ba0d-831658a56f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae6ecf-d071-4412-81bc-5c1ff884c752",
   "metadata": {},
   "source": [
    "## Aggregation Functions:\n",
    "* **count:** specify a specific column to count, or all the columns by using count(*) or count(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "490271c1-60aa-4803-a8ed-5cfd6a41f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|           92764|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2356b64-43c3-4b8f-b330-55740644cab6",
   "metadata": {},
   "source": [
    "* **countDistinct:** Sometimes, the total number is not relevant; rather, itâ€™s the number of unique groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f0c1246-414d-467f-bd8f-f6784b663ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     3116|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f101f-e99d-41d9-ac3b-04f499660451",
   "metadata": {},
   "source": [
    "* **approx_count_distinct** Often, we find ourselves working with large datasets and the exact distinct count is irrelevant.\n",
    "There are times when an approximation to a certain degree of accuracy will work just fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c14046-2834-4af4-8615-14d2b4f58e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            2631|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11874e8b-d4eb-4041-b415-4d5cdb201fdf",
   "metadata": {},
   "source": [
    "* **first and last:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1ab3dbc-1a2b-4342-a133-147a373323cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          20886|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e9c9b-1ea0-49f4-a9dc-409186480d5b",
   "metadata": {},
   "source": [
    "* **max and min:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ded5d5-b233-401f-aa55-e57485ac8a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(StockCode)|max(StockCode)|\n",
      "+--------------+--------------+\n",
      "|         10002|             m|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"StockCode\"), max(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78142e-33a6-456e-a354-7cc21568cb66",
   "metadata": {},
   "source": [
    "* **sum:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f5f2e9e-0deb-44af-9881-06bf9cd8fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|       809925|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5f470-43c8-47c0-afcd-2ec8a763a16e",
   "metadata": {},
   "source": [
    "* **sumDistinct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aca82881-6643-43d7-aadd-56e402b8047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 48673|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c6cf3-97bf-4ef5-92d1-a40b2f227e62",
   "metadata": {},
   "source": [
    "* **avg:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98bd32f2-dd1c-41ff-a43f-6351f7fd46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|   avg(Quantity)|  mean(Quantity)|\n",
      "+----------------+----------------+\n",
      "|8.73102712259066|8.73102712259066|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, expr\n",
    "df.select(avg(\"Quantity\"), expr(\"mean(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462db067-327e-4486-8728-40e58ee689da",
   "metadata": {},
   "source": [
    "* **Variance and Standard Deviation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0ece886-5282-45d4-b0b1-0cfb0d3fb62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+--------------------+\n",
      "|121867.36033221691|   349.0950591632842|\n",
      "+------------------+--------------------+\n",
      "\n",
      "+------------------+---------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+---------------------+\n",
      "|121868.67408188361|   349.09694080854337|\n",
      "+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show()\n",
    "df.select(var_samp(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70902573-8f68-47fa-877a-5f8b68e55a90",
   "metadata": {},
   "source": [
    "* **Skewness and kurtosis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4f170da-5f92-4f78-b354-45e3efd1976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(QUantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.18925049467871705|44043.177602281416|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"QUantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e371f4-4514-497a-a691-f85d3845211e",
   "metadata": {},
   "source": [
    "* **Covariance and Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74cd42e3-8072-443b-a6af-8d7a20c36807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     0.004510386981990214|            2515.7715775622664|              2515.799221884859|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a14304-5690-4caa-9d96-7d69fd297b92",
   "metadata": {},
   "source": [
    "* **Aggregating to Complex Types:**\n",
    "In Spark, you can perform aggregations not just of numerical values using formulas, you can also\n",
    "perform them on complex types. For example, we can collect a list of values present in a given\n",
    "column or only the unique values by collecting to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87f22b50-a24e-4d8f-9965-9060cf516ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6009e3-1b1f-4960-9d36-e3fa0f08c13f",
   "metadata": {},
   "source": [
    "___\n",
    "## Grouping:\n",
    "Thus far, we have performed only DataFrame-level aggregations. A more common task is to\n",
    "perform calculations based on groups in the data. This is typically done on $categorical\\ data$ forwhich we group our data on one column and perform some calculations on the other columns\n",
    "that end up in that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ff324c1-9308-4008-a8c2-7bc17c1cc78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50703375-a58a-4a2d-97da-08e540b347f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca32199-1870-41f3-b49e-9e973f33d314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
