{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f68e04-e9cf-4da6-81e5-f873e12c0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415f96e7-9117-459d-954f-13e440b889ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b83fba-ab6f-4c3d-ae8b-dfef9a84dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a spark context from a sparksession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aaf769-779f-4eaa-a0d9-15e92322e4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating RDDs:\n",
    "\n",
    "**1) Interoperating Between DataFrames and RDDs**\n",
    "Because Python doesn’t have Datasets —it has only DataFrames— you will get an RDD of type\n",
    "Row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6117d772-8ab3-4f27-813d-dbde65afc23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(50).rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ee921-6a37-4cb9-89fc-c07e5f2e246d",
   "metadata": {},
   "source": [
    "To operate on this data, you will need to convert this Row object to the correct data type or extract\n",
    "values out of it, as shown in the example that follows. This is now an RDD of type Row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb94ae62-31b3-4a91-b8cd-2f13ec3c10d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(50).toDF(\"id\").rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32618356-3c58-44f8-847b-07c0ed6a3e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dataframe out of rdd\n",
    "spark.range(50).rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4bebb-737d-40c8-b1e8-6a1d55a99cf0",
   "metadata": {},
   "source": [
    "**2) From a Local Collection** To create an RDD from a collection, you will need to use the $parallelize$ method on a $SparkContext$ (within a SparkSession). \\\n",
    "$This\\ turns\\ a\\ single\\ node\\ collection\\ into\\ a\\ parallel\\ collection.$\\\n",
    "we can also explicitly state the number of partitions into which you would like to distribute this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7241ca-a1d2-429c-a939-9a4f8269043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    ".split(\" \")\n",
    "words = sc.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc14cab-d7d9-42d2-8639-38035d3053b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myWords'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An additional feature is that you can then name this RDD to \n",
    "# show up in the Spark UI according to a given name:\n",
    "words.setName(\"myWords\")\n",
    "words.name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f6087-803d-41e2-9285-eda975a4a4a2",
   "metadata": {},
   "source": [
    "**3) FROM DATA SOURCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c87ae28e-cbc5-4158-bc2e-1d969e54426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.textFile(\"/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c57b6-c67b-4aa2-aab7-05ef5ee340ce",
   "metadata": {},
   "source": [
    "---\n",
    "## Manipualting RDDs:\n",
    "You manipulate RDDs in much the same way that you manipulate DataFrames. \n",
    "\n",
    "### Transformations:\n",
    "For the most part, many transformations mirror the functionality that you find in the Structured\n",
    "APIs. Just as you do with DataFrames and Datasets, you specify transformations on one RDD to\n",
    "create another. In doing so, we define an RDD as a dependency to another along with some\n",
    "manipulation of the data contained in that RDD.\n",
    "\n",
    "`words.show() --> Error, 'RDD' object has no attribute 'show'`\n",
    "\n",
    "* **Distinct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c0daa15-f155-4a82-ba29-3d6ec76a77d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440b407-0ded-48f1-8554-73c5da5680db",
   "metadata": {},
   "source": [
    "* **Filter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8d82801-b125-4514-a0c5-24bf0f91a7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Simple']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def startsWithS(individual):\n",
    "    return individual.startswith(\"S\")\n",
    "\n",
    "words.filter(lambda word: startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5b7ca-6c18-426d-b6ec-c6f6232ff7f3",
   "metadata": {},
   "source": [
    "* **Map:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a23567ed-e9fa-42ec-89b5-b33ab5a54a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 'S', True),\n",
       " ('The', 'T', False),\n",
       " ('Definitive', 'D', False),\n",
       " ('Guide', 'G', False),\n",
       " (':', ':', False),\n",
       " ('Big', 'B', False),\n",
       " ('Data', 'D', False),\n",
       " ('Processing', 'P', False),\n",
       " ('Made', 'M', False),\n",
       " ('Simple', 'S', True)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2 = words.map(lambda word: (word, word[0], word.startswith(\"S\")))\n",
    "words2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a28c2fe9-ab48-4bbf-a11a-be510e4d58f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 'S', True), ('Simple', 'S', True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2.filter(lambda record: record[2]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2622b7a-bd4c-46e2-af7b-2287a9ee5089",
   "metadata": {},
   "source": [
    "* **FlatMap:** Sometimes, each\n",
    "current row should return multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd4b23ae-a5b2-434a-891a-a3d965d500eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'p', 'a', 'r', 'k']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.flatMap(lambda word: word).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b4e7b-5d35-439f-abb9-099f73ae0670",
   "metadata": {},
   "source": [
    "* **Sort:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1652cc2d-b965-49ea-8673-e1d2027437b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Definitive', 'Processing', 'Simple', 'Spark', 'Guide']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sortBy(lambda word: len(word) * -1).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c8143-7fe5-4c5e-b303-784858bdaed6",
   "metadata": {},
   "source": [
    "* **Random Splits:**\n",
    "We can also randomly split an RDD into an Array of RDDs by using the randomSplit method,\n",
    "which accepts an Array of weights and a random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9b22b27-92a0-4b1c-8c9d-0d3888662758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PythonRDD[65] at RDD at PythonRDD.scala:53,\n",
       " PythonRDD[66] at RDD at PythonRDD.scala:53]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.randomSplit([0.5, 0.5])\n",
    "# words.randomSplit([0.5, 0.5])[0].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e83ff-4fbd-4ce7-86ab-651c7b109acb",
   "metadata": {},
   "source": [
    "### Actions:\n",
    "we specify actions to kick off our specified transformations. Actions either collect data to the driver or write to an external data source.\n",
    "\n",
    "* **reduce:** used to specify a function to “reduce” an RDD of any kind of value to one value.\\\n",
    "For instance, given a set of numbers, you can reduce this to its sum by specifying a function that takes as input two values and reduces them into one. If you have experience in functional programming, this should not be a new concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ab0684f-4ddb-48d0-a9d6-547371778af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 21)).reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0df58bb0-200d-4e3f-8a45-415ae48c1b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Processing'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The longest word in a string\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        return leftWord\n",
    "    else:\n",
    "        return rightWord\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414cf69-2c2e-4339-b0ea-a2f28e7b07ba",
   "metadata": {},
   "source": [
    "* **count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab44b5a9-a305-4370-8977-09d4cbfb91b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83165559-6fa7-4f1e-b2a9-62750d0ab40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countApprox\n",
    "confidence = 0.80\n",
    "timeoutMilliseconds = 400\n",
    "words.countApprox(timeoutMilliseconds, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a1a340b-0d09-4f9f-8ad4-90ab0d0cbd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countApproxDistinct\n",
    "words.countApproxDistinct(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "435403ef-6ff5-42fb-babb-7f19f8533034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Spark': 1,\n",
       "             'The': 1,\n",
       "             'Definitive': 1,\n",
       "             'Guide': 1,\n",
       "             ':': 1,\n",
       "             'Big': 1,\n",
       "             'Data': 1,\n",
       "             'Processing': 1,\n",
       "             'Made': 1,\n",
       "             'Simple': 1})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByValue\n",
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc4347-fbaa-4816-9cd8-5e14d0a59801",
   "metadata": {},
   "source": [
    "* **first:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb57b693-c64e-488d-851e-56f70f270c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spark'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f4fd2-96da-49c8-af86-523fa51e4968",
   "metadata": {},
   "source": [
    "* **max and min:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e846503e-4e22-46ce-a33c-cba545110fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1, 20)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0590cadc-5e2f-4d2a-94a6-d31d245f23bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 20)).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1883b6-8dd1-4d49-ada6-b5a1a856afae",
   "metadata": {},
   "source": [
    "* **take** take and its derivative methods take a number of values from your RDD. This works by first\n",
    "scanning one partition and then using the results from that partition to estimate the number of\n",
    "additional partitions needed to satisfy the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb6eabd8-280f-44da-b867-df62bdfd7dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'The', 'Definitive', 'Guide', ':']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba4fc32a-a8b5-4abb-962f-2bfcf8752a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'Big', 'Data', 'Definitive', 'Guide']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2580ce1a-acfa-4685-ad7e-3845d62eba3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Spark', 'Simple', 'Processing', 'Made']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a96b673-b267-4932-b77e-a3761f1747ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'Definitive', 'Data', 'The', 'Definitive', 'Spark']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withReplacement = True\n",
    "numberToTake = 6\n",
    "randomSeed = 100\n",
    "words.takeSample(withReplacement, numberToTake, randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a068b-275b-459c-88e6-013c88bf532b",
   "metadata": {},
   "source": [
    "### Saving Files:\n",
    "* **saveAsTextFile:**\n",
    "To save to a text file, you just specify a path and optionally a compression codec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe29355f-fc92-49d7-98b2-e726d9a23bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words.saveAsTextFile(\"..\\Book_Exercises\\data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c1733-0af5-4063-b661-b98f6f5b7bb2",
   "metadata": {},
   "source": [
    "### Caching:\n",
    "We can either cache or persist an RDD. By default, cache and persist only handle data in memory. We can name it if we use the $setName$ function that we referenced previously in this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a272cae5-96dd-46d9-8e8c-079310003eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myWords ParallelCollectionRDD[25] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62a5d9-b2be-4670-9884-24038232c649",
   "metadata": {},
   "source": [
    "We can specify a storage level as any of the storage levels in the singleton object:\n",
    "org.apache.spark.storage.StorageLevel, which are combinations of memory only; disk\n",
    "only; and separately, off heap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7844fb8-86d9-48fa-908d-185489740c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d1383-cc32-4c19-b3ac-d3b1249bae22",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "One feature not available in the DataFrame API is the concept of checkpointing. \\\n",
    "**Checkpointing is the act of saving an RDD to disk** so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. \\\n",
    "**This is similar to caching except that it’s not stored in memory, only disk. This can be helpful whenperforming iterative computation, similar to the use cases for caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2683f44-7e38-47dd-bc16-a0c447353ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"Book_Exercises/data\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22d4a4-b42c-4a1c-a7d0-81de9a31c555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
